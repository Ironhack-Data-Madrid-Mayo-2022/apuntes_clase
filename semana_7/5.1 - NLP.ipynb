{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1- NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP trata de aplicaciones que entiendan nuestro idioma, reconocimiento de voz, traducción, comprensión semántica, análisis de sentimiento.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usos**\n",
    "\n",
    "+ Motores de búsqueda\n",
    "+ Feed de redes sociales\n",
    "+ Asistentes de voz \n",
    "+ Filtros de span\n",
    "+ Chatbots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Librerías**\n",
    "\n",
    "+ NLTK\n",
    "+ Spacy\n",
    "+ TFIDF\n",
    "+ OpenNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La dificultad del NLP está en varios niveles:\n",
    "\n",
    "+ Ambigüedad:\n",
    "\n",
    "  * Nivel léxico: por ejemplo, varios significados\n",
    "  * Nivel referencial: anáforas, metáforas, etc...\n",
    "  * Nivel estructural: la semántica es necesaria para entender la estructura de una oración\n",
    "  * Nivel pragmático: dobles sentidos, ironía, humor\n",
    "  \n",
    "+ Detección de espacios\n",
    "+ Recepción imperfecta: acentos, -ismos, OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El proceso es similar que en USL, primero se vectorizan las palabras y después se miden sus distancias/similitudes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Godfather',\n",
       " 'The Shawshank Redemption',\n",
       " \"Schindler's List\",\n",
       " 'Raging Bull',\n",
       " 'Casablanca',\n",
       " \"One Flew Over the Cuckoo's Nest\",\n",
       " 'Gone with the Wind',\n",
       " 'Citizen Kane',\n",
       " 'The Wizard of Oz',\n",
       " 'Titanic']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lista de 100 peliculas\n",
    "\n",
    "titles=open('../data/title_list.txt').read().split('\\n')[:100]\n",
    "\n",
    "titles[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Plot  [edit]  [  [  edit  edit  ]  ]  \\n  On the day of his only daughter's wedding, Vito Corleone h\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synopsis=open('../data/synopses_list.txt').read().split('\\n BREAKS HERE')[:100]\n",
    "\n",
    "synopsis[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/iudh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "import spacy\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%python -m spacy download en_core_web_sm\n",
    "#%pip install spacy-lookups-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "stop_words_en=set(stopwords.words('english') + list(STOP_WORDS) + list(nlp.Defaults.stop_words) + ['edit', 'plot'])\n",
    "\n",
    "stop_words=stop_words_en \n",
    "\n",
    "parser=English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(sentence):\n",
    "    \n",
    "    tokens=parser(sentence)\n",
    "    \n",
    "    filtered_tokens=[]\n",
    "\n",
    "    for word in tokens:\n",
    "        \n",
    "        lemma=nlp(str(word))[0].lemma_.lower().strip()\n",
    "        \n",
    "        if lemma not in stop_words and re.search('^[a-zA-Z]+$', lemma):\n",
    "            filtered_tokens.append(lemma)\n",
    "            \n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 143 ms, sys: 4.76 ms, total: 147 ms\n",
      "Wall time: 157 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['day',\n",
       " 'daughter',\n",
       " 'wedding',\n",
       " 'vito',\n",
       " 'corleone',\n",
       " 'hear',\n",
       " 'request',\n",
       " 'role',\n",
       " 'godfather',\n",
       " 'new',\n",
       " 'york',\n",
       " 'crime',\n",
       " 'family',\n",
       " 'vito',\n",
       " 'young',\n",
       " 'son']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spacy_tokenizer(synopsis[0][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF (term frequency inverse document frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(synopsis[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer(min_df=0.15, tokenizer=spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tfidf_matrix=tfidf.fit_transform(synopsis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix.shape, len(synopsis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(str(tfidf_matrix[0]).split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.DataFrame(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms=tfidf.get_feature_names_out()\n",
    "\n",
    "terms[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os dejo la kata\n",
    "\n",
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distancias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity as cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distancias=1-cos(tfidf_matrix)\n",
    "\n",
    "distancias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(distancias).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import pylab as plt\n",
    "from IPython.display import set_matplotlib_formats\n",
    "\n",
    "%matplotlib inline\n",
    "set_matplotlib_formats('svg')\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap=UMAP(n_neighbors=5, random_state=42)\n",
    "\n",
    "emb=umap.fit_transform(distancias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(emb[:, 0], emb[:, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN\n",
    "\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan=DBSCAN(eps=0.8, min_samples=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan.fit(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDBSCAN\n",
    "\n",
    "from hdbscan import HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(HDBSCAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan=HDBSCAN(min_cluster_size=5)\n",
    "\n",
    "clusters=hdbscan.fit_predict(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(emb[:, 0], emb[:, 1], c=clusters);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### titulos de los clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_titles_from_cluster(c):\n",
    "    return pd.Series(titles)[clusters==c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_titles_from_cluster(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df=pd.DataFrame(tfidf_matrix.toarray(), columns=terms)\n",
    "\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_cluster(c):\n",
    "    return tfidf_df[clusters==c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_df_from_cluster(2).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_df_from_cluster(2).describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words=get_df_from_cluster(-1).T.sum(axis=1).sort_values(ascending=False)\n",
    "\n",
    "top_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_titles_from_cluster(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP_es "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download es_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('es_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec=nlp('hola me llamo pepito').vector.sum()\n",
    "\n",
    "doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec=(nlp('hola').vector + nlp('me').vector + nlp('llamo').vector + nlp('pepito').vector).sum()/4\n",
    "\n",
    "word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### similitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp('quiero saludar a todos los alumnos de Ironhack, que pasa alegres').similarity(nlp('hola a todos los alumnos'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_1=spacy_tokenizer('quiero saludar a todos los alumnos de Ironhack, que pasa alegres')\n",
    "token_2=spacy_tokenizer('hola a todos los alumnos')\n",
    "\n",
    "token_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simil(t1, t2):\n",
    "    return nlp(' '.join(t1)).similarity(nlp(' '.join(t2)))  # similitud es entre -1 y 1, por el valor del coseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_simil(token_1, token_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordClouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langdetect\n",
    "#!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# librerias\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "\n",
    "import string\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "\n",
    "from spacy.lang.es import Spanish\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words_sp=set(stopwords.words('spanish') + ['haber'])\n",
    "stop_words_en=set(stopwords.words('english'))\n",
    "\n",
    "stop_words=stop_words_sp | stop_words_en \n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(frase):\n",
    "    \n",
    "    if detect(frase)=='en':  # si esta en ingles...\n",
    "        nlp=spacy.load('en_core_web_sm')\n",
    "        parser=English()\n",
    "        \n",
    "    elif detect(frase)=='es': # si esta en castellano...\n",
    "        nlp=spacy.load('es_core_news_md')\n",
    "        parser=Spanish()\n",
    "    else:\n",
    "        return 'No es ni castellano ni ingles..'\n",
    "    \n",
    "    \n",
    "    tokens=parser(frase)\n",
    "    \n",
    "    clean_tokens=[]\n",
    "    \n",
    "    for e in tokens:\n",
    "        \n",
    "        lema=nlp(str(e))[0].lemma_.lower().strip()\n",
    "        \n",
    "        if lema not in stop_words and re.search('^[a-zA-Z]+$', lema):\n",
    "            \n",
    "            clean_tokens.append(lema)\n",
    "            \n",
    "    return ' '.join(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud(df, col):\n",
    "    \n",
    "    wordcloud=WordCloud(width=1600,\n",
    "                        height=400,\n",
    "                        stopwords=stop_words,\n",
    "                        colormap='Spectral').generate(' '.join([e for e in df[col]]))\n",
    "    \n",
    "    plt.figure(figsize=(15, 10), facecolor='k')\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.savefig('images/wordcloud.png', facecolor='k', bbox_inches='tight')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df=pd.DataFrame(synopsis, columns=['text'])\n",
    "\n",
    "df.text=df.text.apply(tokenizer)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud(df, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "txt=open('../data/conde.txt').read().split('\\n BREAKS HERE')[:100]\n",
    "\n",
    "df2=pd.DataFrame(txt, columns=['text'])\n",
    "\n",
    "df2.text=df2.text.apply(tokenizer)\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud(df2, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(WordCloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mascara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagen con mascara\n",
    "\n",
    "Image.open('images/vino.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vino_mask=np.array(Image.open('images/vino.png'))\n",
    "\n",
    "vino_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformacion de la mascara\n",
    "\n",
    "def transformacion(val):\n",
    "    if val==0:\n",
    "        return 255\n",
    "    else:\n",
    "        return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_vino_mask=np.ndarray((vino_mask.shape[0], vino_mask.shape[1]), np.int32)\n",
    "\n",
    "\n",
    "for i in range(len(vino_mask)):\n",
    "    t_vino_mask[i]=list(map(transformacion, vino_mask[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=WordCloud(background_color='white',\n",
    "                   max_words=1000,\n",
    "                   mask=t_vino_mask,\n",
    "                   stopwords=stop_words,\n",
    "                   contour_width=3,\n",
    "                   contour_color='firebrick').generate(' '.join([e for e in df.text]))\n",
    "\n",
    "\n",
    "w.to_file('images/copa&botella.png')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 10), facecolor='k')\n",
    "plt.imshow(w)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ejemplo con todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(lst):  # ahora entra una lista\n",
    "    \n",
    "    en=0\n",
    "    es=0\n",
    "    \n",
    "    for txt in lst:\n",
    "        try:\n",
    "            txt=str(txt)\n",
    "            if detect(txt)=='en':    # si el texto esta en ingles...\n",
    "                en+=1\n",
    "                \n",
    "                nlp=spacy.load('en_core_web_sm')\n",
    "                parser=English()\n",
    "                tokens=parser(txt)\n",
    "\n",
    "                tokens_en=[]\n",
    "\n",
    "                for word in tokens:\n",
    "                    lemma=nlp(str(word))[0].lemma_.lower().strip()\n",
    "                    if lemma not in STOP_WORDS and re.search('^[a-zA-Z]+$', lemma):\n",
    "                        tokens_en.append(lemma)\n",
    "\n",
    "            elif detect(txt)=='es':   # si el texto esta en castellano...\n",
    "                es+=1\n",
    "                \n",
    "                nlp=spacy.load('es_core_news_md')\n",
    "                parser=Spanish()\n",
    "                tokens=parser(txt)\n",
    "\n",
    "                tokens_es=[]\n",
    "\n",
    "                for word in tokens:\n",
    "                    lemma=nlp(str(word))[0].lemma_.lower().strip()\n",
    "                    if lemma not in STOP_WORDS and re.search('^[a-zA-Z]+$', lemma):\n",
    "                        tokens_es.append(lemma)\n",
    "\n",
    "            else:\n",
    "                print ('No se reconoce idioma (EN / ES)...')\n",
    "        \n",
    "        except:\n",
    "            print ('ERROR...')\n",
    "            continue\n",
    "    \n",
    "    \n",
    "    return ' '.join(tokens_en), ' '.join(tokens_es), en, es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mix_txt=synopsis+txt\n",
    "\n",
    "tokens=tokenizer(mix_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud en ingles\n",
    "\n",
    "serie_en=pd.DataFrame({'en': tokens[0]}, index=[0])\n",
    "\n",
    "wordcloud(serie_en, 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud en castellano\n",
    "\n",
    "serie_es=pd.DataFrame({'es': tokens[1]}, index=[0])\n",
    "\n",
    "wordcloud(serie_es, 'es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news=pd.read_csv('data/noticias.csv')\n",
    "\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.tail(2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download es_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_core_es='es_core_news_md'\n",
    "\n",
    "spacy_core_en='en_core_web_sm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner(spacy_core, data):\n",
    "    \n",
    "    nlp=spacy.load(spacy_core)\n",
    "    \n",
    "    frases=list(nlp(data).sents)  # frases, sentencias\n",
    "    \n",
    "    entidades=displacy.render(nlp(str(frases)), style='ent')\n",
    "    \n",
    "    return entidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner(spacy_core_es, news.text[150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner(spacy_core_en, synopsis[3][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers (creacion de texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install tensorflow\n",
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generador=pipeline('text-generation', \n",
    "                   model='EleutherAI/gpt-neo-125M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crea_texto(generador, texto, min_long=20):\n",
    "    \n",
    "    return generador(texto, do_sample=True, min_length=min_long)[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crea_texto(generador, 'data science is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crea_texto(generador, 'what planet is this')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crea_texto(generador, 'humans are')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crea_texto(generador, 'el monstruo de las galletas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crea_texto(generador, 'molt be noi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crea_texto(generador, 'rellampagos asgaya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crea_texto(generador, 'humans are')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(generador)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crea_texto(generador, 'what the fuck asshole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1657819950299,
   "trusted": true
  },
  "kernelspec": {
   "display_name": "ironcomes",
   "language": "python",
   "name": "ironcomes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
